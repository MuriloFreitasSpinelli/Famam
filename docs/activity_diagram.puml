@startuml src_v4_activity_diagram
!theme plain
skinparam activityFontSize 11
skinparam swimlaneFontSize 12

title Music Generation Pipeline - Activity Diagram

|#LightBlue|User|
|#LightGreen|Client Pipeline|
|#LightYellow|Data Preprocessing|
|#LightPink|Model Training|
|#LightCyan|Music Generation|

' ============================================================================
' START
' ============================================================================
|User|
start
:Select operation;

switch (Operation?)
case (Build Dataset)
    goto BuildDataset
case (Train Model)
    goto TrainModel
case (Generate Music)
    goto GenerateMusic
case (Exit)
    stop
endswitch

' ============================================================================
' BUILD DATASET FLOW
' ============================================================================
label BuildDataset
|User|
:Provide MIDI directory path;
:Specify output path;
:Choose encoder type\n(event/remi/multitrack);

|Client Pipeline|
:Create PipelineConfig;
:Initialize MusicPipeline;

|Data Preprocessing|
:Scan MIDI directory;
:Load genre.tsv mapping\n(if available);

partition "Process Each MIDI File" {
    while (More MIDI files?) is (yes)
        :Load MIDI with muspy;
        :Extract tracks;
        :Extract metadata\n(genre, artist);

        if (Meets criteria?) then (yes)
            :Register genres in Vocabulary;
            :Register instruments in Vocabulary;
            :Create MusicEntry\n(music, genre, song_id);
            :Add to MusicDataset;
        else (no)
            :Skip file;
        endif
    endwhile (no)
}

:Build Vocabulary stats\n(instrument_to_songs,\ngenre_to_instruments);

|Client Pipeline|
:Create encoder based on type;

if (encoder_type?) then (multitrack)
    :Create MultiTrackEncoder;
else (event/remi)
    :Create EventEncoder\nor REMIEncoder;
endif

|Data Preprocessing|
:Save dataset to HDF5;
note right
  Saves:
  - Metadata (resolution, max_seq_length)
  - Vocabulary (genres, instruments)
  - All MusicEntry objects (pickled)
end note

|Client Pipeline|
:Print dataset statistics;
:Return MusicDataset;

|User|
:Dataset ready!;
goto End

' ============================================================================
' TRAIN MODEL FLOW
' ============================================================================
label TrainModel
|User|
:Provide dataset path;
:Configure training\n(epochs, batch_size, d_model, etc.);
:Choose model type\n(transformer/lstm);
:Enable multitrack?\n(all instruments together);

|Client Pipeline|
:Load MusicDataset from HDF5;
:Extract Vocabulary;
:Create appropriate encoder;

|Data Preprocessing|
if (multitrack mode?) then (yes)
    partition "Multi-Track Encoding" {
        :For each MusicEntry;
        :Collect all tracks;
        :Sort events by time;
        :Interleave all instruments;
        :Create token sequence:\n[BOS, genre, bar, pos,\ninst, pitch, dur, vel, ...];
    }
    :Call to_multitrack_dataset();
else (no)
    partition "Single-Track Encoding" {
        :For each track in each entry;
        :Convert track to events;
        :Create token sequence:\n[BOS, genre, inst, events...];
    }
    :Call to_tensorflow_dataset();
endif

:Split into train/val/test\n(by song to prevent leakage);
:Create tf.data.Dataset\nwith batching & shuffling;

|Model Training|
:Create TrainingConfig;
:Initialize Trainer;

partition "Build Model" {
    if (model_type?) then (transformer)
        :Build TransformerModel\n- Token embedding\n- N x TransformerBlock\n- Output projection;
    else (lstm)
        :Build LSTMModel\n- Token embedding\n- N x LSTM layers\n- Output projection;
    endif
}

:Create optimizer\n(Adam with LR schedule);
:Create loss function\n(MaskedSparseCategoricalCrossentropy);
:Create metrics\n(MaskedAccuracy);
:Setup callbacks\n(EarlyStopping, Checkpointing,\nTensorBoard);

partition "Training Loop" {
    while (More epochs?) is (yes)
        :Iterate over batches;

        partition "Forward Pass" {
            :Embed input tokens;
            :Apply model layers;
            :Project to vocab logits;
            :Compute masked loss;
        }

        partition "Backward Pass" {
            :Compute gradients;
            :Clip gradients;
            :Update weights;
        }

        :Log training metrics;

        if (Validation epoch?) then (yes)
            :Evaluate on val_dataset;
            :Check early stopping;
            :Save checkpoint if best;
        endif
    endwhile (no)
}

|Client Pipeline|
:Create ModelBundle\n(model + encoder + config);
:Save to HDF5 + _model directory;
:Print training summary;

|User|
:Model trained!;
goto End

' ============================================================================
' GENERATE MUSIC FLOW
' ============================================================================
label GenerateMusic
|User|
:Provide model path;
:Choose genre;
:Choose generation mode\n(single/multi/multitrack/drums);
:Set parameters\n(temperature, top_k, top_p,\nmin_notes, min_bars);

|Client Pipeline|
:Load ModelBundle;
:Extract model, encoder, config;

if (multitrack mode?) then (yes)
    :Create MultiTrackGenerator;
else (no)
    :Create MusicGenerator;
endif

|Music Generation|
partition "Create Start Tokens" {
    :Build conditioning sequence:\n[BOS, genre_token, ...];

    if (multitrack?) then (no)
        :Add instrument_token;
    endif

    :Add initial bar_token;
}

partition "Autoregressive Generation" {
    while (length < max_length\nAND not EOS?) is (continue)
        :Forward pass through model;
        :Get logits for next token;

        partition "Sampling" {
            :Apply temperature scaling;
            :Apply top-k filtering;
            :Apply top-p (nucleus) filtering;
            :Sample from distribution;
        }

        :Append token to sequence;

        if (token == EOS?) then (yes)
            break
        endif
    endwhile (stop)
}

:Return generated tokens;

partition "Decode to Music" {
    :Convert tokens to events;

    if (multitrack?) then (yes)
        :Group events by instrument;
        :Create multiple tracks;
    else (no)
        :Create single track;
    endif

    :Convert events to muspy.Note objects;
    :Set timing from bar/position or time_shift;
    :Create muspy.Music object;
}

|Client Pipeline|
:Validate generation quality;

partition "Quality Check" {
    :Count notes per track;
    :Calculate duration in bars;

    if (meets min_notes AND min_bars?) then (yes)
        :Accept generation;
    else (no)
        :Retry (up to max_retries);
    endif
}

if (include rule-based drums?) then (yes)
    |Music Generation|
    :Create DrumPatternGenerator;
    :Select pattern\n(rock, alternative, etc.);
    :Generate drum track\nwith fills and crashes;
    :Add humanization\n(velocity variation);
    :Merge with generated tracks;
endif

|Client Pipeline|
:Set tempo and resolution;
:Write MIDI file with muspy;
:Print generation stats;

|User|
:Music generated!;
:Play or edit MIDI file;

label End
|User|
:Continue or exit;

if (Continue?) then (yes)
    :Select operation;
    switch (Operation?)
    case (Build Dataset)
        goto BuildDataset
    case (Train Model)
        goto TrainModel
    case (Generate Music)
        goto GenerateMusic
    case (Exit)
        stop
    endswitch
else (no)
    stop
endif

@enduml
