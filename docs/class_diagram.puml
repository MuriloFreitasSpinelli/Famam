@startuml src_v4_class_diagram
!theme plain
skinparam classAttributeIconSize 0
skinparam classFontSize 11
skinparam packageFontSize 12
skinparam linetype ortho

title Music Generation Pipeline - Class Diagram

' ============================================================================
' DATA PREPROCESSING MODULE
' ============================================================================
package "data_preprocessing" {

    package "encoders" {
        abstract class BaseEncoder {
            + {abstract} vocab_size : int
            + {abstract} special_tokens : dict
            + pad_token_id : int
            + bos_token_id : int
            + eos_token_id : int
            --
            + {abstract} encode_track(track, genre_id, instrument_id, max_length) : EncodedSequence
            + {abstract} decode_tokens(tokens, skip_special) : List[Tuple]
            + {abstract} create_conditioning_tokens(genre_id, instrument_id) : ndarray
            + {abstract} get_state() : dict
            + {static} from_state(state) : BaseEncoder
            + create_labels(token_ids) : ndarray
            + pad_sequence(tokens, max_length) : Tuple
            + is_special_token(token) : bool
        }

        class EncodedSequence <<dataclass>> {
            + token_ids : ndarray
            + attention_mask : ndarray
            + metadata : dict
        }

        class EventVocabulary <<dataclass>> {
            - NOTE_ON_OFFSET : int = 0
            - NOTE_OFF_OFFSET : int = 128
            - TIME_SHIFT_OFFSET : int = 256
            - VELOCITY_OFFSET : int = 356
            - PAD_TOKEN : int = 388
            - BOS_TOKEN : int = 389
            - EOS_TOKEN : int = 390
            - GENRE_OFFSET : int = 391
            + num_genres : int
            + num_instruments : int
            --
            + encode_note_on(pitch) : int
            + encode_note_off(pitch) : int
            + encode_time_shift(ticks) : int
            + encode_velocity(velocity) : int
            + encode_genre(genre_id) : int
            + encode_instrument(instrument_id) : int
            + decode_token(token) : Tuple
        }

        class EventEncoder extends BaseEncoder {
            + vocabulary : EventVocabulary
            + encode_velocity : bool
            --
            + track_to_events(track) : List[Tuple]
            + events_to_tokens(events, genre_id, instrument_id) : List[int]
            + decode_to_notes(tokens) : List[Dict]
        }

        class REMIVocabulary <<dataclass>> {
            - BAR_TOKEN : int = 0
            - POSITION_OFFSET : int = 1
            - PITCH_OFFSET : int = 33
            - DURATION_OFFSET : int = 161
            - VELOCITY_OFFSET : int = 225
            - PAD_TOKEN : int = 257
            - BOS_TOKEN : int = 258
            - EOS_TOKEN : int = 259
            + num_genres : int
            + num_instruments : int
            + ticks_per_bar : int
            --
            + encode_bar() : int
            + encode_position(pos) : int
            + encode_pitch(pitch) : int
            + encode_duration(dur) : int
            + decode_token(token) : Tuple
        }

        class REMIEncoder extends BaseEncoder {
            + resolution : int
            + positions_per_bar : int
            + time_signature : Tuple
            + vocabulary : REMIVocabulary
            --
            + track_to_remi_events(track) : List[Tuple]
            + events_to_tokens(events) : List[int]
            + decode_to_notes(tokens) : List[Dict]
        }

        class MultiTrackEvent <<dataclass>> {
            + time : int
            + bar : int
            + position : int
            + instrument_id : int
            + pitch : int
            + duration : int
            + velocity : int
        }

        class MultiTrackEncoder extends BaseEncoder {
            - PAD_TOKEN : int = 0
            - BOS_TOKEN : int = 1
            - EOS_TOKEN : int = 2
            - BAR_OFFSET : int = 4
            - POSITION_OFFSET : int = 68
            - INSTRUMENT_OFFSET : int = 132
            - PITCH_OFFSET : int = 261
            + num_genres : int
            + resolution : int
            + max_bars : int
            + positions_per_bar : int
            --
            + bar_token(bar) : int
            + position_token(pos) : int
            + instrument_token(inst_id) : int
            + pitch_token(pitch) : int
            + encode_music(music, genre_id, max_length) : EncodedSequence
            + decode_to_music(tokens, resolution, tempo) : Music
        }

        EventEncoder *-- EventVocabulary
        REMIEncoder *-- REMIVocabulary
    }

    class Vocabulary {
        + genre_to_id : Dict[str, int]
        + artist_to_id : Dict[str, int]
        + instrument_to_id : Dict[str, int]
        + instrument_to_songs : Dict[int, Set]
        + genre_to_instruments : Dict[str, Set]
        --
        + add_genre(genre) : int
        + get_genre_id(genre) : int
        + get_genre_name(genre_id) : str
        + num_genres : int
        + num_active_instruments : int
        + register_instrument_usage(inst_id, song_id, genre)
        + get_instrument_stats() : Dict
        + get_top_instruments_for_genre(genre, top_n) : List
        + to_dict() : Dict
        + {static} from_dict(data) : Vocabulary
        + save(filepath)
        + {static} load(filepath) : Vocabulary
    }

    class MusicEntry <<dataclass>> {
        + music : Music
        + genre : str
        + song_id : str
    }

    class MusicDataset {
        + resolution : int
        + max_seq_length : int
        + entries : List[MusicEntry]
        + vocabulary : Vocabulary
        --
        + add_entry(entry)
        + add(music, genre, song_id)
        + count_tracks() : int
        + to_tensorflow_dataset(encoder, splits) : Dataset
        + to_multitrack_dataset(encoder, splits) : Dataset
        + save(filepath)
        + {static} load(filepath) : MusicDataset
        + get_stats() : Dict
        + get_instrument_stats(top_n) : List
        + print_instrument_stats(top_n)
    }

    class MusicDatasetConfig <<dataclass>> {
        + name : str
        + input_dirs : List[str]
        + output_path : str
        + resolution : int
        + max_seq_length : int
        + encoder_type : str
        + train_split : float
        + val_split : float
        + quantize : bool
        + segment_length : int
        --
        + ticks_per_bar : int
        + save(path)
        + {static} load(path) : MusicDatasetConfig
    }

    MusicDataset *-- Vocabulary
    MusicDataset *-- "many" MusicEntry
    MusicDataset ..> BaseEncoder : uses
}

' ============================================================================
' MODEL TRAINING MODULE
' ============================================================================
package "model_training" {

    package "architectures" {
        abstract class BaseMusicModel <<keras.Model>> {
            + vocab_size : int
            + max_seq_length : int
            + d_model : int
            + dropout_rate : float
            + token_embedding : Embedding
            + output_projection : Dense
            --
            + {abstract} build_layers()
            + {abstract} call(inputs, training) : Tensor
            + {abstract} get_config() : Dict
            + embed_tokens(input_ids, training) : Tensor
            + generate(start_tokens, max_length, temperature, top_k, top_p, eos_token_id) : Tensor
            + summary_str() : str
            + {static} get_causal_attention_mask(seq_length) : Tensor
        }

        class RelativePositionalEmbedding <<keras.Layer>> {
            + max_relative_position : int
            + embedding_dim : int
            + embeddings : Variable
            --
            + call(length) : Tensor
        }

        class RelativeMultiHeadAttention <<keras.Layer>> {
            + d_model : int
            + num_heads : int
            + head_dim : int
            + wq, wk, wv, wo : Dense
            + relative_pos_emb : RelativePositionalEmbedding
            --
            + call(query, key, value, mask, training) : Tensor
        }

        class TransformerBlock <<keras.Layer>> {
            + attention : RelativeMultiHeadAttention
            + ffn : Sequential
            + norm1, norm2 : LayerNormalization
            + dropout : Dropout
            --
            + call(x, attention_mask, training) : Tensor
        }

        class TransformerModel extends BaseMusicModel {
            + num_layers : int
            + num_heads : int
            + d_ff : int
            + max_relative_position : int
            + use_relative_attention : bool
            + transformer_blocks : List[TransformerBlock]
            + final_norm : LayerNormalization
            --
            + build_layers()
            + call(inputs, training) : Tensor
        }

        class LSTMModel extends BaseMusicModel {
            + lstm_units : Tuple[int]
            + recurrent_dropout : float
            + bidirectional : bool
            + lstm_layers : List[LSTM]
            + dropout_layers : List[Dropout]
            + final_norm : LayerNormalization
            --
            + build_layers()
            + call(inputs, training) : Tensor
        }

        TransformerModel *-- "many" TransformerBlock
        TransformerBlock *-- RelativeMultiHeadAttention
        RelativeMultiHeadAttention *-- RelativePositionalEmbedding
    }

    class TrainingConfig <<dataclass>> {
        + model_name : str
        + model_type : str
        + max_seq_length : int
        + d_model : int
        + dropout_rate : float
        .. Transformer ..
        + num_layers : int
        + num_heads : int
        + d_ff : int
        + use_relative_attention : bool
        .. LSTM ..
        + lstm_units : Tuple
        + bidirectional : bool
        .. Training ..
        + batch_size : int
        + epochs : int
        + learning_rate : float
        + warmup_steps : int
        + label_smoothing : float
        + use_early_stopping : bool
        + use_checkpointing : bool
        --
        + save(path)
        + {static} load(path) : TrainingConfig
        + summary() : str
        + {static} get_transformer_small() : TrainingConfig
        + {static} get_transformer_medium() : TrainingConfig
    }

    class TransformerLRSchedule <<LearningRateSchedule>> {
        + d_model : float
        + warmup_steps : float
        --
        + __call__(step) : float
    }

    class MaskedSparseCategoricalCrossentropy <<Loss>> {
        + pad_token_id : int
        + label_smoothing : float
        --
        + call(y_true, y_pred) : float
    }

    class MaskedAccuracy <<Metric>> {
        + pad_token_id : int
        + correct : Variable
        + total : Variable
        --
        + update_state(y_true, y_pred)
        + result() : float
        + reset_state()
    }

    class Trainer {
        + config : TrainingConfig
        + encoder : BaseEncoder
        + model : BaseMusicModel
        + history : Dict
        + strategy : Strategy
        + num_replicas : int
        + global_batch_size : int
        --
        - _setup_strategy() : Strategy
        - _prepare_dataset(dataset, batch_size) : Dataset
        - _build_model() : BaseMusicModel
        - _setup_callbacks() : List[Callback]
        - _setup_optimizer()
        - _setup_loss_and_metrics()
        + train(train_dataset, val_dataset, output_dir) : Tuple
    }

    class ModelMetadata <<dataclass>> {
        + model_name : str
        + model_type : str
        + vocab_size : int
        + max_seq_length : int
        + d_model : int
        + num_layers : int
        + num_heads : int
        + d_ff : int
        + lstm_units : int
        + encoder_type : str
        + pad_token_id : int
        + bos_token_id : int
        + eos_token_id : int
    }

    class ModelBundle {
        + model : BaseMusicModel
        + encoder : BaseEncoder
        + config : TrainingConfig
        + model_name : str
        + metadata : ModelMetadata
        --
        + save(filepath)
        + {static} load(filepath, custom_objects) : ModelBundle
        + vocab_size : int
        + max_seq_length : int
        + model_type : str
        + create_start_tokens(genre_id, instrument_id) : ndarray
        + generate(genre_id, instrument_id, ...) : ndarray
        + decode_tokens(tokens) : List
        + summary() : str
    }

    Trainer o-- TrainingConfig
    Trainer o-- BaseEncoder
    Trainer --> BaseMusicModel : creates
    Trainer ..> TransformerLRSchedule : uses
    Trainer ..> MaskedSparseCategoricalCrossentropy : uses
    Trainer ..> MaskedAccuracy : uses

    ModelBundle *-- BaseMusicModel
    ModelBundle *-- BaseEncoder
    ModelBundle *-- TrainingConfig
    ModelBundle *-- ModelMetadata
}

' ============================================================================
' MUSIC GENERATION MODULE
' ============================================================================
package "music_generation" {

    class GenerationConfig <<dataclass>> {
        + max_length : int = 1024
        + temperature : float = 1.0
        + top_k : int = 50
        + top_p : float = 0.9
        + resolution : int = 24
        + tempo : float = 120.0
        + min_notes : int = 10
        + min_bars : int = 4
        + max_retries : int = 3
        + seed : int
    }

    class MusicGenerator {
        + model : BaseMusicModel
        + encoder : BaseEncoder
        + config : GenerationConfig
        --
        + {static} from_bundle(bundle, config) : MusicGenerator
        + {static} from_bundle_path(filepath, config) : MusicGenerator
        + generate_tokens(genre_id, instrument_id, ...) : ndarray
        + generate_events(genre_id, instrument_id, ...) : List[Tuple]
        + generate_notes(genre_id, instrument_id, ...) : List[Note]
        + generate_track(genre_id, instrument_id, ...) : Track
        + generate_music(genre_id, instrument_ids, ...) : Music
        + generate_midi(output_path, genre_id, ...) : Music
        + validate_track(track) : Dict
    }

    enum DrumNote {
        KICK = 36
        SNARE = 38
        HIHAT_CLOSED = 42
        HIHAT_OPEN = 46
        TOM_LOW = 45
        TOM_MID = 47
        TOM_HIGH = 50
        CRASH_1 = 49
        RIDE = 51
    }

    class DrumPattern <<dataclass>> {
        + name : str
        + beats_per_bar : int
        + subdivisions : int
        + kick : List[int]
        + snare : List[int]
        + hihat : List[int]
        + accent_positions : List[int]
        --
        + total_positions : int
    }

    class DrumPatternGenerator {
        + resolution : int
        + default_velocity : int
        + humanize : bool
        + humanize_amount : int
        + patterns : Dict[str, DrumPattern]
        --
        - _get_velocity(base, is_accent) : int
        - _position_to_time(pos, bar, pattern) : int
        + generate_bar(pattern, bar_number, add_crash, fill_positions) : List[Note]
        + generate_fill(pattern, bar_number, fill_length) : List[Note]
        + generate_track(num_bars, pattern_name, ...) : Track
        + generate_intro(num_bars, pattern_name) : Track
        + list_patterns() : List[str]
        + get_pattern_info(pattern_name) : Dict
    }

    class MultiTrackConfig <<dataclass>> {
        + max_length : int = 2048
        + temperature : float = 0.9
        + top_k : int = 50
        + top_p : float = 0.92
        + resolution : int = 24
        + tempo : float = 120.0
        + num_bars : int = 8
        + min_notes_per_track : int = 5
        + min_total_notes : int = 20
        + max_retries : int = 3
        + seed : int
    }

    class MultiTrackGenerator {
        + model : BaseMusicModel
        + encoder : MultiTrackEncoder
        + config : MultiTrackConfig
        --
        + {static} from_bundle(bundle, config) : MultiTrackGenerator
        + generate_tokens(genre_id, instruments, ...) : ndarray
        + generate_music(genre_id, instruments, ...) : Music
        + generate_midi(output_path, genre_id, ...) : Music
        + generate_with_prompt(prompt_music, genre_id, ...) : Music
        + get_generation_stats(music) : Dict
        + print_generation_stats(music)
    }

    MusicGenerator o-- BaseMusicModel
    MusicGenerator o-- BaseEncoder
    MusicGenerator *-- GenerationConfig

    MultiTrackGenerator o-- BaseMusicModel
    MultiTrackGenerator o-- MultiTrackEncoder
    MultiTrackGenerator *-- MultiTrackConfig

    DrumPatternGenerator *-- "many" DrumPattern
    DrumPatternGenerator ..> DrumNote : uses
}

' ============================================================================
' CLIENT MODULE
' ============================================================================
package "client" {

    class PipelineConfig <<dataclass>> {
        .. Paths ..
        + data_dir : str
        + output_dir : str
        + model_dir : str
        .. Dataset ..
        + encoder_type : str
        + resolution : int
        + max_seq_length : int
        + train_split : float
        + val_split : float
        + multitrack : bool
        .. Model ..
        + model_type : str
        + model_name : str
        + d_model : int
        + num_layers : int
        + num_heads : int
        + d_ff : int
        + dropout : float
        + lstm_units : int
        .. Training ..
        + batch_size : int
        + epochs : int
        + learning_rate : float
        + warmup_steps : int
        .. Generation ..
        + temperature : float
        + top_k : int
        + top_p : float
        --
        + to_dataset_config() : MusicDatasetConfig
        + to_training_config(vocab_size) : TrainingConfig
        + to_generation_config() : GenerationConfig
        + save(filepath)
        + {static} load(filepath) : PipelineConfig
    }

    class MusicPipeline {
        + config : PipelineConfig
        + encoder : BaseEncoder
        + vocabulary : Vocabulary
        + dataset : MusicDataset
        + model : BaseMusicModel
        + trainer : Trainer
        + generator : MusicGenerator
        + bundle : ModelBundle
        + history : Dict
        --
        .. Dataset ..
        + create_encoder(num_genres, num_instruments) : BaseEncoder
        + build_dataset(midi_dir, output_path, genre_filter) : MusicDataset
        + load_dataset(filepath) : MusicDataset
        + get_tf_datasets(batch_size) : Dict[str, Dataset]
        .. Training ..
        + build_model() : BaseMusicModel
        + train(train_dataset, val_dataset, epochs) : Dict
        + save_model(filepath) : str
        + load_model(filepath) : ModelBundle
        .. Generation ..
        - _ensure_generator() : MusicGenerator
        - _ensure_multitrack_generator() : MultiTrackGenerator
        + generate(genre_id, instrument_id, ...) : ndarray
        + generate_midi(output_path, genre_id, ...) : Music
        + generate_track(genre_id, instrument_id, ...) : Track
        + generate_multitrack(output_path, genre_id, ...) : Music
        + is_multitrack_mode() : bool
        .. Utility ..
        + list_genres() : List[str]
        + get_genre_id(genre_name) : int
        + list_instruments() : List[str]
        + get_instrument_stats(top_n) : List
        + print_instrument_stats(top_n)
        + summary() : str
    }

    class InteractiveShell {
        + config : PipelineConfig
        + pipeline : MusicPipeline
        + running : bool
        --
        + run()
        + execute(line)
        + cmd_help(args)
        + cmd_status(args)
        + cmd_config(args)
        + cmd_set(args)
        + cmd_save(args)
        + cmd_load(args)
        + cmd_dataset(args)
        + cmd_train(args)
        + cmd_generate(args)
        + cmd_genres(args)
        + cmd_instruments(args)
    }

    class MenuCLI {
        + pipeline : MusicPipeline
        + config : PipelineConfig
        + running : bool
        + gen_settings : Dict
        --
        + run()
        + main_menu()
        + dataset_menu()
        + training_menu()
        + generation_menu()
        + settings_menu()
        + info_menu()
        + build_dataset()
        + load_dataset()
        + train_model()
        + load_model()
        + generate_full_song()
        + generate_single_instrument()
        + generate_multi_instrument()
        + generate_multitrack_song()
        + generate_drums_only()
    }

    MusicPipeline *-- PipelineConfig
    MusicPipeline o-- BaseEncoder
    MusicPipeline o-- Vocabulary
    MusicPipeline o-- MusicDataset
    MusicPipeline o-- BaseMusicModel
    MusicPipeline o-- Trainer
    MusicPipeline o-- MusicGenerator
    MusicPipeline o-- ModelBundle

    InteractiveShell *-- PipelineConfig
    InteractiveShell *-- MusicPipeline

    MenuCLI *-- PipelineConfig
    MenuCLI *-- MusicPipeline
}

' ============================================================================
' EXTERNAL DEPENDENCIES
' ============================================================================
package "external" #DDDDDD {
    class "muspy.Music" as Music
    class "muspy.Track" as Track
    class "muspy.Note" as Note
    class "tf.data.Dataset" as TFDataset
    class "keras.Model" as KerasModel
}

MusicEntry o-- Music
MusicDataset ..> TFDataset : creates
BaseMusicModel --|> KerasModel
MusicGenerator ..> Music : creates
MultiTrackGenerator ..> Music : creates
DrumPatternGenerator ..> Track : creates

@enduml
