{
  "model_name": "transformer_large",
  "model_type": "transformer",
  "max_seq_length": 2048,
  "d_model": 768,
  "dropout_rate": 0.1,
  "num_layers": 12,
  "num_heads": 12,
  "d_ff": 3072,
  "use_relative_attention": true,
  "max_relative_position": 512,
  "lstm_units": [512, 512],
  "bidirectional": false,
  "recurrent_dropout": 0.0,
  "batch_size": 4,
  "epochs": 100,
  "learning_rate": 0.0001,
  "warmup_steps": 4000,
  "label_smoothing": 0.1,
  "optimizer": "adam",
  "weight_decay": 0.01,
  "beta_1": 0.9,
  "beta_2": 0.98,
  "epsilon": 1e-09,
  "use_lr_schedule": true,
  "lr_schedule_type": "transformer",
  "use_gradient_clipping": true,
  "gradient_clip_value": 1.0,
  "use_early_stopping": true,
  "early_stopping_patience": 10,
  "early_stopping_min_delta": 0.0001,
  "early_stopping_monitor": "val_loss",
  "use_checkpointing": true,
  "checkpoint_monitor": "val_loss",
  "save_best_only": true,
  "use_tensorboard": true,
  "tensorboard_log_dir": "./logs",
  "output_dir": "./models",
  "save_history": true,
  "save_final_model": true,
  "random_seed": 42,
  "shuffle": true,
  "shuffle_buffer_size": 10000,
  "distribution_strategy": "none",
  "batch_size_per_replica": null
}
